{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from turtlesim_enacter import TurtleSimEnacter # requires ROS\n",
    "from turtlepy_enacter import TurtlePyEnacter\n",
    "import random\n",
    "# from Agent5 import Agent5\n",
    "# from OsoyooCarEnacter import OsoyooCarEnacter\n",
    "ROBOT_IP = \"192.168.4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = 0\n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        return self._action\n",
    "    \n",
    "class AgentBoring(Agent) :\n",
    "    def __init__(self,valences_table,steps_until_bored) :\n",
    "        super().__init__(valences_table)\n",
    "        self.step_until_bored = steps_until_bored\n",
    "        self.step = 0\n",
    "        self.actualAction = 0\n",
    "        self.memory = {}\n",
    "\n",
    "    def guessOutcome(self,action) :\n",
    "        outcomesCount = {}\n",
    "        count = 0\n",
    "        if action not in self.memory :\n",
    "            return 0\n",
    "        \n",
    "        for out in self.memory[action] : \n",
    "            count += 1\n",
    "            if out in outcomesCount : \n",
    "\n",
    "                outcomesCount[out] += 1\n",
    "            else :\n",
    "                outcomesCount[out] = 1\n",
    "\n",
    "        for out in outcomesCount :\n",
    "            outcomesCount[out] = outcomesCount[out] / count\n",
    "        \n",
    "        \n",
    "        rand = random.random()\n",
    "        sum = 0\n",
    "        for out in outcomesCount : \n",
    "            \n",
    "            sum += outcomesCount[out]\n",
    "            if sum > rand :\n",
    "                return out\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    def action(self, outcome) :\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        if self.step > 1 : \n",
    "            if self.actualAction not in self.memory :\n",
    "                self.step += 1 \n",
    "                self.memory[self.actualAction] = [self.actualAction]\n",
    "            else :\n",
    "                self.memory[self.actualAction] += [self.actualAction]\n",
    "                self.step += 1\n",
    "            if self.step % self.step_until_bored != 0 :\n",
    "                self.anticipated_outcome = self.guessOutcome(self.actualAction)\n",
    "                self._action = self.actualAction\n",
    "                return  self._action\n",
    "            else : \n",
    "                self.actualAction = (self.actualAction+1)%2\n",
    "                self.anticipated_outcome = self.guessOutcome(self.actualAction)\n",
    "                self._action = self.actualAction\n",
    "                return self._action\n",
    "\n",
    "        else : \n",
    "            self.step += 1 \n",
    "            self.anticipated_outcome = 0\n",
    "            self._action = self.actualAction\n",
    "            return self._action\n",
    "                # TODO: Implement the agent's anticipation mechanism\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        print(action)\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else : \n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment3:\n",
    "    \"\"\" Environment 3 yields outcome 1 only when the agent alternates actions 0 and 1 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment3 \"\"\"\n",
    "        self.previous_action = 0\n",
    "\n",
    "    def outcome(self, action):\n",
    "        _outcome = 1\n",
    "        if action == self.previous_action:\n",
    "            _outcome = 0\n",
    "        self.previous_action = action\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define the valance of interactions (action, outcome)\n",
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "# TODO Choose an agent\n",
    "a = AgentBoring(valences,4)\n",
    "# a = Agent5(valences)\n",
    "# TODO Choose an environment\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "# e = Environment3()\n",
    "# e = TurtleSimEnacter()\n",
    "# e = TurtlePyEnacter()\n",
    "# e = OsoyooCarEnacter(ROBOT_IP)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" The main loop controlling the interaction of the agent with the environment \"\"\"\n",
    "    outcome = 0\n",
    "    for i in range(20):\n",
    "        action = a.action(outcome)\n",
    "        outcome = e.outcome(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
