{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from turtlesim_enacter import TurtleSimEnacter # requires ROS\n",
    "from turtlepy_enacter import TurtlePyEnacter\n",
    "# from Agent5 import Agent5\n",
    "# from OsoyooCarEnacter import OsoyooCarEnacter\n",
    "ROBOT_IP = \"192.168.4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = 0\n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment3:\n",
    "    \"\"\" Environment 3 yields outcome 1 only when the agent alternates actions 0 and 1 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment3 \"\"\"\n",
    "        self.previous_action = 0\n",
    "\n",
    "    def outcome(self, action):\n",
    "        _outcome = 1\n",
    "        if action == self.previous_action:\n",
    "            _outcome = 0\n",
    "        self.previous_action = action\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define the valance of interactions (action, outcome)\n",
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "# TODO Choose an agent\n",
    "a = Agent(valences)\n",
    "# a = Agent5(valences)\n",
    "# TODO Choose an environment\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "# e = Environment3()\n",
    "# e = TurtleSimEnacter()\n",
    "# e = TurtlePyEnacter()\n",
    "# e = OsoyooCarEnacter(ROBOT_IP)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" The main loop controlling the interaction of the agent with the environment \"\"\"\n",
    "    outcome = 0\n",
    "    for i in range(20):\n",
    "        action = a.action(outcome)\n",
    "        outcome = e.outcome(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
